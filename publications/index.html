<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="Federico Simonetta is musician and computer engineer. From Pavia to Milan through Padua. Here you found my publications, music, software and contacts. Pop in to know me!">
<title>
Publications - Federico Simonetta
</title>


<link rel="shortcut icon" href="https://federicosimonetta.eu.org/img/avatar.ico">








<link rel="stylesheet" href="https://federicosimonetta.eu.org/css/main.min.202eb2a494b9f8d0a0b154015fe6c213c3d44bdf745bece33482defd57380131.css" integrity="sha256-IC6ypJS5&#43;NCgsVQBX&#43;bCE8PUS990W&#43;zjNILe/Vc4ATE=" crossorigin="anonymous" media="screen">



<link rel="stylesheet" href="https://federicosimonetta.eu.org/css/custom.css">


<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://federicosimonetta.eu.org/img/image.jpg"/>

<meta name="twitter:title" content="Publications"/>
<meta name="twitter:description" content="The following are some notable publications that you may be interested in. Here is the full list of publications.
A perceptual measure for evaluating the resynthesis of automatic music transcriptions See more Federico Simonetta, S. Ntalampiras, F. Avanzini
Published in Multimedia Tools and Applications 2022
Full paper Web Site Source-code Abstract This study focuses on the perception of music performances when contextual factors, such as room acoustics and instrument, change. We propose to distinguish the concept of “performance” from the one of “interpretation”, which expresses the “artistic intention”."/>

<meta property="og:title" content="Publications" />
<meta property="og:description" content="The following are some notable publications that you may be interested in. Here is the full list of publications.
A perceptual measure for evaluating the resynthesis of automatic music transcriptions See more Federico Simonetta, S. Ntalampiras, F. Avanzini
Published in Multimedia Tools and Applications 2022
Full paper Web Site Source-code Abstract This study focuses on the perception of music performances when contextual factors, such as room acoustics and instrument, change. We propose to distinguish the concept of “performance” from the one of “interpretation”, which expresses the “artistic intention”." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://federicosimonetta.eu.org/publications/" /><meta property="og:image" content="https://federicosimonetta.eu.org/img/image.jpg"/><meta property="article:section" content="" />

<meta property="og:site_name" content="Federico Simonetta" />



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-142387081-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "CollectionPage",
  "headline": "Publications",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https:\/\/federicosimonetta.eu.org\/publications\/"
  },
  "image": {
    "@type": "ImageObject",
    "url": "https:\/\/federicosimonetta.eu.org\/img\/avatar.png",
    "width":  1075 ,
    "height":  531 
  },
  "genre": "page",
  
  "wordcount":  561 ,
  "url": "https:\/\/federicosimonetta.eu.org\/publications\/",
  
  
  "license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.",
  "publisher": {
    "@type": "Organization",
    "name": "Federico Simonetta",
    "logo": {
      "@type": "ImageObject",
      "url": "https:\/\/federicosimonetta.eu.org\/img\/avatar.ico",
      "width":  128 ,
      "height":  128 
    }
  },
  "author": {
    "@type": "Person",
    "name": "Federico Simonetta"
  },
  "description": ""
}
</script>


<link rel="canonical" href="https://federicosimonetta.eu.org/publications/">




  <style> html, body { display: none; } </style>
  <script>
  window.addEventListener('load', (event) => {
    var color = "hsl(" + Math.floor(Math.random() * 255).toString() + "," + Math.floor(Math.random() * 100).toString() + "%,30%)";
    document.body.style.backgroundColor = color;
    document.body.style.display = "initial";
    document.getElementsByTagName("html")[0].style.backgroundColor = color;
    document.getElementsByTagName("html")[0].style.display = "initial"
    });
  </script>


    

    
    
    
    <title>
        
        Publications
        
    </title>
</head>

<body>
    <div class="wrap">
        
        <div class="section top-menu">
  <p>
    

    

        
        
          <a href="https://federicosimonetta.eu.org/about">who am I?</a>
          
            -
          
        
          <a href="https://federicosimonetta.eu.org/publications">publications</a>
          
            -
          
        
          <a href="https://federicosimonetta.eu.org/post">random posts</a>
          
            -
          
        
          <a href="https://federicosimonetta.eu.org/links">not so random links</a>
          
            -
          
        
          <a href="https://federicosimonetta.eu.org/software">unsoftware</a>
          
            -
          
        
          <a href="https://federicosimonetta.eu.org/music">noise</a>
          
            -
          
        
          <a href="https://federicosimonetta.eu.org/teaching">don&#39;t tell this anyone...</a>
          
            -
          
        
          <a href="https://federicosimonetta.eu.org/notes/allnotes">...tell everyone this one</a>
          
        
    
        <p>
            <a href="https://federicosimonetta.eu.org/">
                home
            </a>
        </p>
    
</p>
<hr />

</div>
        
    
        <h1 class="section" id="title">Publications</h1>

        <div class="section" id="content"><p>The following are some notable publications that you may be interested in.
Here is <a href="https://federicosimonetta.eu.org/publications-full">the full list of publications</a>.</p>
<p><img src="https://federicosimonetta.eu.org/img/listening_test_screenshot.png" alt="Screenshot of the Listening Test"></p>
<h3 id="a-perceptual-measure-for-evaluating-the-resynthesis-of-automatic-music-transcriptions">A perceptual measure for evaluating the resynthesis of automatic music transcriptions</h3>

<details class="expand">
    <summary class="expand-label" style="cursor: pointer;" >
        <span>
        
    	
    	See more
    	
    	</span>
    </summary>
    <p><em>Federico Simonetta, S. Ntalampiras, F. Avanzini</em></p>
<p>Published in Multimedia Tools and Applications 2022</p>
<ul>
<li><a href="https://arxiv.org/pdf/2202.12257.pdf">Full paper</a></li>
<li><a href="https://limunimi.github.io/MIA/">Web Site</a></li>
<li><a href="https://github.com/LIMUNIMI/PerceptualEvaluation">Source-code</a></li>
</ul>
<h5 id="abstract">Abstract</h5>
<p>This study focuses on the perception of music performances when contextual
factors, such as room acoustics and instrument, change. We propose to
distinguish the concept of “performance” from the one of “interpretation”,
which expresses the “artistic intention”. Towards assessing this distinction,
we carried out an experimental evaluation where 91 subjects were invited to
listen to various audio recordings created by resynthesizing MIDI data obtained
through Automatic Music Transcription (AMT) systems and a sensorized acoustic
piano. During the resynthesis, we simulated different contexts and asked
listeners to evaluate how much the interpretation changes when the context
changes. Results show that: (1) MIDI format alone is not able to completely
grasp the artistic intention of a music performance; (2) usual objective
evaluation measures based on MIDI data present low correlations with the
average subjective evaluation. To bridge this gap, we propose a novel measure
which is meaningfully correlated with the outcome of the tests. In addition, we
investigate multimodal machine learning by providing a new score-informed AMT
method and propose an approximation algorithm for the p-dispersion problem.</p>

</details>

<hr>
<p><img src="https://federicosimonetta.eu.org/img/audio_to_score_alignment.png" alt="Results of the A2S method"></p>
<h3 id="audio-to-score-alignment-using-deep-automatic-music-transcription">Audio-to-Score Alignment Using Deep Automatic Music Transcription</h3>

<details class="expand">
    <summary class="expand-label" style="cursor: pointer;" >
        <span>
        
    	
    	See more
    	
    	</span>
    </summary>
    <p><em>Federico Simonetta, S. Ntalampiras, F. Avanzini</em></p>
<p>Published in MMSP 2021</p>
<ul>
<li><a href="http://arxiv.org/pdf/2107.12854">Full paper</a></li>
<li><a href="https://github.com/LIMUNIMI/MMSP2021-Audio2ScoreAlignment">Source-code</a></li>
</ul>
<h5 id="abstract">Abstract</h5>
<p>Audio-to-score alignment (A2SA) is a multimodal task consisting in the
alignment of audio signals to music scores. Recent literature confirms the
benefits of Automatic Music Transcription (AMT) for A2SA at the frame-level. In
this work, we aim to elaborate on the exploitation of AMT Deep Learning (DL)
models for achieving alignment at the note-level. We propose a method which
benefits from HMM-based score-to-score alignment and AMT, showing a remarkable
advancement beyond the state-of-the-art. We design a systematic procedure to
take advantage of large datasets which do not offer an aligned score. Finally,
we perform a thorough comparison and extensive tests on multiple datasets.</p>

</details>

<hr>
<p><img src="https://federicosimonetta.eu.org/img/cnn_melody_identification.png" alt="Example of melody identification"></p>
<h3 id="a-convolutional-approach-to-melody-line-identification-in-symbolic-scores">A Convolutional Approach to Melody Line Identification in Symbolic Scores</h3>

<details class="expand">
    <summary class="expand-label" style="cursor: pointer;" >
        <span>
        
    	
    	See more
    	
    	</span>
    </summary>
    <p><em>Federico Simonetta, Carlos Eduardo Cancino-Chacón, S. Ntalampiras, G. Widmer</em></p>
<p>Published in ISMIR 2022</p>
<ul>
<li><a href="https://arxiv.org/pdf/1906.10547.pdf">Full paper</a></li>
<li><a href="https://arxiv.org/pdf/1906.10547.pdf">Website</a></li>
<li><a href="https://github.com/LIMUNIMI/Symbolic-Melody-Identification">Source-code</a></li>
</ul>
<h5 id="abstract">Abstract</h5>
<p>In many musical traditions, the melody line is of primary significance in a
piece. Human listeners can readily distinguish melodies from accompaniment;
however, making this distinction given only the written score &ndash; i.e. without
listening to the music performed &ndash; can be a difficult task. Solving this task
is of great importance for both Music Information Retrieval and musicological
applications. In this paper, we propose an automated approach to identifying
the most salient melody line in a symbolic score. The backbone of the method
consists of a convolutional neural network (CNN) estimating the probability
that each note in the score (more precisely: each pixel in a piano roll
encoding of the score) belongs to the melody line. We train and evaluate the
method on various datasets, using manual annotations where available and solo
instrument parts where not. We also propose a method to inspect the CNN and to
analyze the influence exerted by notes on the prediction of other notes; this
method can be applied whenever the output of a neural network has the same size
as the input.</p>

</details>

</div>

        <div class="section footer"><hr />
<em>Fluid thoughts against categories</em>
</div>
    </div>
</body>

</html>
