<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="Federico Simonetta is a machine learning researcher. Interested in music, sound, and digital humantities, here you found my publications, music, software and contacts. Pop in to know me!">
<title>
Publications - Federico Simonetta
</title>
 
<link rel="shortcut icon" href="https://federicosimonetta.eu.org/img/avatar.ico">

 





<link rel="stylesheet" href="https://federicosimonetta.eu.org/css/main.min.cab2f05221f5866a68a01806a27f2c580ae5d3556730a118b4ccb74e9970c16c.css" integrity="sha256-yrLwUiH1hmpooBgGon8sWArl01VnMKEYtMy3TplwwWw=" crossorigin="anonymous" media="screen">



<link rel="stylesheet" href="https://federicosimonetta.eu.org/css/custom.css">



<link rel="preconnect" href="https://fonts.googleapis.com" />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link
  href="https://fonts.googleapis.com/css2?family=Fira+Sans+Condensed:wght@300&family=Fira+Sans:wght@200&display=swap"
  rel="stylesheet"
/>

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://federicosimonetta.eu.org/img/image.jpg" /><meta name="twitter:title" content="Publications"/>
<meta name="twitter:description" content="Here is the full list of my publications .
The following are some notable publications that you may be interested in.
A perceptual measure for evaluating the resynthesis of automatic music transcriptions See more Federico Simonetta, S. Ntalampiras, F. Avanzini
Published in Multimedia Tools and Applications 2022
Full paper Web Site Source-code Abstract This study focuses on the perception of music performances when contextual factors, such as room acoustics and instrument, change."/>
 <meta property="og:title" content="Publications" />
<meta property="og:description" content="Here is the full list of my publications .
The following are some notable publications that you may be interested in.
A perceptual measure for evaluating the resynthesis of automatic music transcriptions See more Federico Simonetta, S. Ntalampiras, F. Avanzini
Published in Multimedia Tools and Applications 2022
Full paper Web Site Source-code Abstract This study focuses on the perception of music performances when contextual factors, such as room acoustics and instrument, change." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://federicosimonetta.eu.org/publications/" /><meta property="og:image" content="https://federicosimonetta.eu.org/img/image.jpg" /><meta property="article:section" content="" />

<meta property="og:site_name" content="Federico Simonetta" />



<script
  async
  src="https://www.googletagmanager.com/gtag/js?id=G-91GZC4XQBY"
></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());

  gtag("config", "G-91GZC4XQBY");
</script>

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "CollectionPage",
  "headline": "Publications",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https:\/\/federicosimonetta.eu.org\/publications\/"
  },
  "image": {
    "@type": "ImageObject",
    "url": "https:\/\/federicosimonetta.eu.org\/img\/avatar.png",
    "width":  1075 ,
    "height":  531 
  },
  "genre": "page",
  
  "wordcount":  613 ,
  "url": "https:\/\/federicosimonetta.eu.org\/publications\/",
  
  
  "license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.",
  "publisher": {
    "@type": "Organization",
    "name": "Federico Simonetta",
    "logo": {
      "@type": "ImageObject",
      "url": "https:\/\/federicosimonetta.eu.org\/img\/avatar.ico",
      "width":  128 ,
      "height":  128 
    }
  },
  "author": {
    "@type": "Person",
    "name": "Federico Simonetta"
  },
  "description": ""
}
</script>
 
<link rel="canonical" href="https://federicosimonetta.eu.org/publications/" />
  

    

    
    
    
    <title>
        
        Publications
        
    </title>
</head>

<body>
    <div class="wrap">
        
        <div class="section top-menu">
<p>
  
  <a href="https://federicosimonetta.eu.org/">home</a>
   &#183;     
  <a href="https://federicosimonetta.eu.org/about">who am I?</a>
   -  
  <a href="https://federicosimonetta.eu.org/publications">publications</a>
   -  
  <a href="https://federicosimonetta.eu.org/post">random posts</a>
   -  
  <a href="https://federicosimonetta.eu.org/links">not so random links</a>
   -  
  <a href="https://federicosimonetta.eu.org/music">noise</a>
   -  
  <a href="https://federicosimonetta.eu.org/teaching">don&#39;t tell this anyone...</a>
   

  
  
  
  
  
  
</p>
<hr />

</div>
        
    
        <h1 class="section" id="title">Publications</h1>

        <div class="section" id="content"><!-- Remove the margins before the title -->
<style>
#content h3 {
margin: -2em 0em 0em;
}
</style>
<p>Here is <a href="https://www.zotero.org/fsimonetta" target="_blank" >the full list of my publications</a>
.</p>
<!-- Here is [the full list of publications](/publications-full). -->
<p>The following are some notable publications that you may be interested in.</p>
<p><img src="https://federicosimonetta.eu.org/img/listening_test_screenshot.png" alt="Screenshot of the Listening Test"></p>
<h3 id="a-perceptual-measure-for-evaluating-the-resynthesis-of-automatic-music-transcriptions">A perceptual measure for evaluating the resynthesis of automatic music transcriptions</h3>

<details class="expand">
    <summary class="expand-label" style="cursor: pointer;" >
        <span>
        
    	
    	See more
    	
    	</span>
    </summary>
    <p><em>Federico Simonetta, S. Ntalampiras, F. Avanzini</em></p>
<p>Published in Multimedia Tools and Applications 2022</p>
<ul>
<li><a href="https://arxiv.org/abs/2202.12257" target="_blank" >Full paper</a>
</li>
<li><a href="https://limunimi.github.io/MIA/" target="_blank" >Web Site</a>
</li>
<li><a href="https://github.com/LIMUNIMI/PerceptualEvaluation" target="_blank" >Source-code</a>
</li>
</ul>
<h5 id="abstract">Abstract</h5>
<p>This study focuses on the perception of music performances when contextual
factors, such as room acoustics and instrument, change. We propose to
distinguish the concept of “performance” from the one of “interpretation”,
which expresses the “artistic intention”. Towards assessing this distinction,
we carried out an experimental evaluation where 91 subjects were invited to
listen to various audio recordings created by resynthesizing MIDI data obtained
through Automatic Music Transcription (AMT) systems and a sensorized acoustic
piano. During the resynthesis, we simulated different contexts and asked
listeners to evaluate how much the interpretation changes when the context
changes. Results show that: (1) MIDI format alone is not able to completely
grasp the artistic intention of a music performance; (2) usual objective
evaluation measures based on MIDI data present low correlations with the
average subjective evaluation. To bridge this gap, we propose a novel measure
which is meaningfully correlated with the outcome of the tests. In addition, we
investigate multimodal machine learning by providing a new score-informed AMT
method and propose an approximation algorithm for the p-dispersion problem.</p>

</details>

<hr>
<p><img src="https://federicosimonetta.eu.org/img/acoustics_specific_screenshot.jpg" alt="Acoustics-specific strategies improve velocity prediction"></p>
<h3 id="acoustics-specific-piano-velocity-estimation">Acoustics-specific Piano Velocity Estimation</h3>

<details class="expand">
    <summary class="expand-label" style="cursor: pointer;" >
        <span>
        
    	
    	See more
    	
    	</span>
    </summary>
    <p><em>Federico Simonetta, S. Ntalampiras, F. Avanzini</em></p>
<p>Published in MMSP 2022</p>
<ul>
<li><a href="https://arxiv.org/abs/2203.16294" target="_blank" >Full paper</a>
</li>
<li><a href="https://limunimi.github.io/MIA/" target="_blank" >Web Site</a>
</li>
<li><a href="https://github.com/LIMUNIMI/ContextAwareAMT/releases/tag/phdthesis" target="_blank" >Source-code</a>
</li>
</ul>
<h5 id="abstract">Abstract</h5>
<p>Motivated by the state-of-art psychological research, we note that a piano performance
transcribed with existing Automatic Music Transcription (AMT) methods cannot be
successfully resynthesized without affecting the artistic content of the performance.
This is due to 1) the different mappings between MIDI parameters used by different
instruments, and 2) the fact that musicians adapt their way of playing to the
surrounding acoustic environment. To face this issue, we propose a methodology to build
acoustics-specific AMT systems that are able to model the adaptations that musicians
apply to convey their interpretation. Specifically, we train models tailored for virtual
instruments in a modular architecture that takes as input an audio recording and the
relative aligned music score, and outputs the acoustics-specific velocities of each
note. We test different model shapes and show that the proposed methodology generally
outperforms the usual AMT pipeline which does not consider specificities of the
instrument and of the acoustic environment. Interestingly, such a methodology is
extensible in a straightforward way since only slight efforts are required to train
models for the inference of other piano parameters, such as pedaling.</p>

</details>

<hr>
<p><img src="https://federicosimonetta.eu.org/img/optimizing_feature_extraction_screenshot.jpg" alt="Benchmarks of feature extraction tools"></p>
<h3 id="optimizing-feature-extraction-for-symbolic-music">Optimizing Feature Extraction for Symbolic Music</h3>

<details class="expand">
    <summary class="expand-label" style="cursor: pointer;" >
        <span>
        
    	
    	See more
    	
    	</span>
    </summary>
    <p><em>Federico Simonetta, Ana Llorens, Martín Serrano, Eduardo García-Portugués, Álvaro
Torrente</em></p>
<p>Published in ISMIR 2023</p>
<ul>
<li><a href="https://arxiv.org/abs/2307.05107" target="_blank" >Full paper</a>
</li>
<li><a href="https://github.com/DIDONEproject/music_symbolic_features/" target="_blank" >Source-code</a>
</li>
</ul>
<h5 id="abstract">Abstract</h5>
<p>This paper presents a comprehensive investigation of existing feature extraction tools
for symbolic music and contrasts their performance to determine the feature set that
best characterizes the musical style of a given music score. In this regard, we propose
a novel feature extraction tool, named musif, and evaluate its efficacy on various
repertoires and file formats, including MIDI, MusicXML, and **kern. Musif approximates
existing tools such as jSymbolic and music21 in terms of computational efficiency while
attempting to enhance the usability for custom feature development. The proposed tool
also enhances classification accuracy when combined with other feature sets. We
demonstrate the contribution of each feature set and the computational resources they
require. Our findings indicate that the optimal tool for feature extraction is a
combination of the best features from each tool rather than a single one. To facilitate
future research in music information retrieval, we release the source code of the tool
and benchmarks.</p>

</details>

</div>

        <div class="section footer"><hr />
<em>Fluid thoughts against categories</em>
</div>
    </div>
</body>

</html>
